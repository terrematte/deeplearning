{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd098e2-49c6-43df-9151-050694970b75",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px;\">**Chat with Your Documents Using GPT & LangChain**</span>\n",
    "\n",
    "\n",
    "**Objectives:** \n",
    "- *Learn how to effectively load & store documents using LangChain*\n",
    "- *Build a retrieval augmented generation pipeline for querying data*\n",
    "- *Build a question-answering bot that answers questions based on your documents*\n",
    "\n",
    "You can learn more about the LangChain library in the following links:\n",
    "* [How to Make Large Language Models Play Nice with Your Software Using LangChain](https://www.kdnuggets.com/how-to-make-large-language-models-play-nice-with-your-software-using-langchain)\n",
    "* [6 Problems of LLMs That LangChain is Trying to Assess](https://www.kdnuggets.com/6-problems-of-llms-that-langchain-is-trying-to-assess)\n",
    "\n",
    "Let's start by understanding our main goal:\n",
    "\n",
    "First: \n",
    "- Take a set of PDFs. \n",
    "- Break them into pieces of texts. \n",
    "- Embed them into a vectorized representation. \n",
    "- Store them into a vector database. (FAISS, CHROMA, PINECONE...)\n",
    "- Once the vectors are persistend in the ddbb, we can get queries, embed them and find a similar chunk vectors. \n",
    "- The chunks are ranked according to how relevant they are to the question and are used to contextualize our LLM. \n",
    "\n",
    "**IMPORTANT:** The LLM doesn't really know what PDFs have. We take advantage of the LLM model to generate NLP answers and provide it with a question and a context to generate an accurate answer. \n",
    "\n",
    "![Structure_main](Structure_main.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42239f47-628c-4f50-aa9d-ee482177dd0c",
   "metadata": {},
   "source": [
    "# Install Imports and API Keys\n",
    "\n",
    "We need to make sure our environment has the following packages: \n",
    "\n",
    "- Create a conda enviroment for compatibillity between packages; \n",
    "- Install `langchain` \n",
    "- Install `tiktoken`, `wikipedia`, `pypdf`, `faiss-cpu`, `pinecone-client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430d0f5-7341-464e-8d2e-c1f322edeb76",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53044,
    "lastExecutedAt": 1709641405903,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install openai==0.27.1\n!pip install langchain==0.0.184\n!pip install tiktoken\n!pip install wikipedia\n!pip install pypdf\n!pip install faiss-cpu\n!pip install pinecone-client",
    "outputsMetadata": {
     "0": {
      "height": 578,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "#!conda create -n conda_llm python==3.8.10\n",
    "!conda init\n",
    "!conda activate conda_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install httpx==0.27.2\n",
    "%pip install openai==1.55.3\n",
    "%pip install langchain==0.2.11\n",
    "%pip install langchain-openai==0.1.19\n",
    "%pip install langchain-community==0.2.10\n",
    "%pip install langchain-experimental==0.0.63\n",
    "%pip install tiktoken==0.7.0\n",
    "%pip install wikipedia==1.4.0\n",
    "%pip install pypdf==4.1.0\n",
    "%pip install faiss-cpu==1.7.4\n",
    "%pip install pinecone-client==3.1.0\n",
    "%pip install pandas==1.5.1\n",
    "%pip install matplotlib==3.6.3\n",
    "%pip install python-dotenv==1.0.0\n",
    "%pip install sentence-transformers==2.2.2\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install matplotlib-inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db4425-1415-4422-878e-fc555b2fbe0b",
   "metadata": {},
   "source": [
    "Before starting, make sure you have avaiable: \n",
    "- OpenAI API Key\n",
    "- Pinecone API Key and environment. \n",
    "\n",
    "To get our API keys, we can set them in an .env document and load them into our environement using the 'load_dotenv()' command or define them directly. \n",
    "- To obtain OpenAI API Keys, you can follow the instructions [here](https://medium.com/forcodesake/a-step-by-step-guide-to-getting-your-api-key-2f6ee1d3e197). \n",
    "- To obtain Pinecone API keys, you can follow the instructions [here](https://medium.com/forcodesake/pinecone-api-chatgpt-artificial-intelligence-4332de128dd5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8360cb-c7a0-4976-8fe9-5482d14b4495",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1591,
    "lastExecutedAt": 1704816818752,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Basics\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom dotenv import load_dotenv\n\n# LangChain Training\n# LLM\nfrom langchain.llms import OpenAI\n\n# Document Loader\nfrom langchain.document_loaders import PyPDFLoader \n\n# Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# Embedding\nfrom langchain.embeddings import OpenAIEmbeddings \n\n# Vector DataBase\nfrom langchain.vectorstores import FAISS, Pinecone # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-based and permanent. \n\n# Chains\n#from langchain.chains.question_answering import load_qa_chain\n#from langchain.chains import ConversationalRetrievalChain"
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Training\n",
    "# LLM\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Document Loader\n",
    "from langchain.document_loaders import PyPDFLoader \n",
    "\n",
    "# Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "# Tokenizer\n",
    "from transformers import GPT2TokenizerFast  \n",
    "\n",
    "# Embedding\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "\n",
    "# Vector DataBase\n",
    "from langchain.vectorstores import FAISS, Pinecone # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-based and permanent. \n",
    "\n",
    "# Chains\n",
    "#from langchain.chains.question_answering import load_qa_chain\n",
    "#from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d70449-d284-43a6-ae31-df6e62ca1302",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1704816847276,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# We can directly upload our keys using a .env\n#load_dotenv()\n\nimport os\n\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\npinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\npinecone_env_key = os.environ[\"PINECONE_ENV_KEY\"]\n\n# Alternatively, you can set the API keys as follows:\n#OPENAI_API_KEY   = \"sk-\"\n#PINECONE_API_KEY = \"34...\"\n#PINECONE_ENV_KEY = \"gcp-starter\""
   },
   "outputs": [],
   "source": [
    "# We can directly upload our keys using a .env\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "pinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "pinecone_env_key = os.environ[\"PINECONE_ENV_KEY\"]\n",
    "\n",
    "# Alternatively, you can set the API keys as follows:\n",
    "#OPENAI_API_KEY   = \"sk-\"\n",
    "#PINECONE_API_KEY = \"34...\"\n",
    "#PINECONE_ENV_KEY = \"gcp-starter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a853ff5-0446-4f29-8e78-66fbe290c7f9",
   "metadata": {},
   "source": [
    "\n",
    "# PART 1: LANGCHAIN BASICS\n",
    "\n",
    "\n",
    "🎯 **Objective:** Understand what is the LangChain library and all the elements that are required to generate a simple pipeline to query out documents. \n",
    "\n",
    "### **What is LangChain?**\n",
    "> LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "LangChain makes the hardest parts of working with AI models easier in two main ways:\n",
    "\n",
    "1. **Data-aware** - Bring external data, such as your files, other applications, and API data, to your LLMs\n",
    "2. **Agentic** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next. \n",
    "\n",
    "### **Why LangChain?**\n",
    "1. **Components** - Abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\n",
    "\n",
    "2. **Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together. A structured assembly of components for accomplishing specific higher-level tasks.\n",
    "\n",
    "3. **Speed 🚢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. **Community 👥** - Wonderful discord and community support, meet ups, hackathons, etc.\n",
    "\n",
    "Though the usage of LLMs can be straightforward (text-in, text-out), when trying to build complex applications you'll quickly notice friction points. \n",
    "\n",
    "> LangChain helps with once you develop more complicated application and manage LLMs the way we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f548002-dda8-4d89-9c1e-7057392c11c5",
   "metadata": {},
   "source": [
    "## LangChain Components\n",
    "\n",
    "The LangChain library contains multiple elements to ease the process of building complex applications using LangChain.\n",
    "In this module we will focus mainly in 10 elements:\n",
    "\n",
    "**To load and process our documents**\n",
    "- Document Loaders\n",
    "- Text Splitters\n",
    "- Chat Messages *(Optional)*\n",
    "\n",
    "\n",
    "**To talk with our documents using NLP**\n",
    "- LLM model (GPT, Llama...)\n",
    "- Chains\n",
    "- Natural Language Retrieval\n",
    "- Metadata and Indexes\n",
    "- Memory *(Optional)*\n",
    "\n",
    "**Both Processes**\n",
    "- Text Embedding (OpenAI or Open-source models)\n",
    "- Vector Stores \n",
    "\n",
    "![Structure_basics](Structure_basics.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb81ae-fe1d-4108-a050-2e66c1bbf296",
   "metadata": {},
   "source": [
    "###  **The Model - Large Language Model of our choice**\n",
    "An AI-powered LLM that takes text in and responses text out. \n",
    "The default model is always ada-001, but we can explicitly choose the model of our preference. \n",
    "\n",
    "You can check the list of all avaialble models [here](https://platform.openai.com/docs/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c96d08-9a35-40f5-a248-f0cb74a842fc",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 4360,
    "lastExecutedAt": 1704817021400,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.llms import OpenAI\n\nchatgpt = OpenAI(\n                 model_name = \"gpt-3.5-turbo\", \n                 temperature= 0\n)\n\nprompt=\"Please, tell me some funny jokes\"\n\nprint(chatgpt(prompt))",
    "outputsMetadata": {
     "0": {
      "height": 525,
      "type": "stream"
     },
     "1": {
      "height": 437,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are a few jokes for you:\n",
      "\n",
      "1. Why don't scientists trust atoms? Because they make up everything!\n",
      "\n",
      "2. Why don't skeletons fight each other? They don't have the guts!\n",
      "\n",
      "3. What do you call a bear with no teeth? A gummy bear!\n",
      "\n",
      "4. Why don't eggs tell jokes? Because they might crack up!\n",
      "\n",
      "5. Why did the scarecrow win an award? Because he was outstanding in his field!\n",
      "\n",
      "6. What do you call a fish wearing a crown? King Neptune!\n",
      "\n",
      "7. Why did the bicycle fall over? Because it was two-tired!\n",
      "\n",
      "8. How do you organize a space party? You \"planet\"!\n",
      "\n",
      "9. Why don't scientists trust stairs? Because they're always up to something!\n",
      "\n",
      "10. What do you call a snowman with a six-pack? An abdominal snowman!\n",
      "\n",
      "Remember, humor is subjective, so what may be funny to one person may not be to another. Enjoy!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"), #you can put the key here directy\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Please, tell me some funny jokes\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chat_completion.choices[0].message.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "@tool\n",
    "def multiply(a, b):\n",
    "    \"Multiply to numbers. For any questions about multiplying two numbers, you must use this tool!\"\n",
    "    return a * b\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "tools = [multiply]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "  (\"human\", \"{input}\"),\n",
    "  MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Do you know what is 5 times 6?\"), AIMessage(content=\"Yes!\")]\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "response = agent_executor.invoke({\"input\": \"tell me\", \"chat_history\": chat_history})    # the question itself is based on prior history\n",
    "print(response[\"output\"])        # 5 times 6 is 30.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039302b-02bb-499c-8637-f4be1bd2e34c",
   "metadata": {},
   "source": [
    "### **Chat Messages**\n",
    "LangChain allows us to segmentate prompts into three main types.(System, Human, AI)\n",
    "\n",
    "* **System** - Helpful background context that tell the AI its high-level behavior.\n",
    "* **Human** - Messages that represent the user input. \n",
    "* **AI** - Messages that show the response of the AI model, they work as examples to the model. \n",
    "\n",
    "\n",
    "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a6c86bf-542a-43f3-a1e7-221ed9bf6918",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 954,
    "lastExecutedAt": 1704817116502,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nhigh_level_behavior = \"\"\"\n                       You are an AI bot that help people decide where to travel. \n                       Always recommend three destination with a short sentence for each.\n                      \"\"\"\n\nresponse = chatgpt(\n    [\n        SystemMessage(content=high_level_behavior),\n        AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n        HumanMessage(content=\"Where should I travel next?\"),\n    ]\n)\n\nprint(response.content)",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     },
     "1": {
      "height": 57,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That depends on your preferences! Could you please let me know what type of destination you are looking for? For example, do you prefer beach destinations, historical cities, or natural landscapes?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n",
    "                  temperature=0\n",
    "                 )\n",
    "\n",
    "high_level_behavior = \"\"\"\n",
    "                       You are an AI bot that help people decide where to travel. \n",
    "                       Always recommend three destination with a short sentence for each.\n",
    "                      \"\"\"\n",
    "\n",
    "response = chatgpt.invoke(\n",
    "    [\n",
    "        SystemMessage(content=high_level_behavior),\n",
    "        AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n",
    "        HumanMessage(content=\"Where should I travel next?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab221be1-08fd-4942-b06e-5945cce7b148",
   "metadata": {},
   "source": [
    "You can also pass more chat history with responses from the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035d9b68-ff7c-48fe-98e4-76961df7c480",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3077,
    "lastExecutedAt": 1704817121541,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "response = chatgpt(\n        [\n            SystemMessage(content=high_level_behavior),\n            AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n            HumanMessage(content=\"Where should I travel next?\"),\n            SystemMessage(content=\"What do you enjoy doing?\"),\n            HumanMessage(content=\"I love going to Museums?\"),\n        ]\n    )\n\nprint(response.content)",
    "outputsMetadata": {
     "0": {
      "height": 227,
      "type": "stream"
     },
     "1": {
      "height": 217,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your love for museums, here are three destinations you might enjoy:\n",
      "\n",
      "1. Paris, France: Known as the \"City of Museums,\" Paris is home to world-renowned museums like the Louvre, Musée d'Orsay, and Centre Pompidou, offering a rich collection of art, history, and culture.\n",
      "\n",
      "2. Florence, Italy: Florence is a treasure trove of art and history, with museums like the Uffizi Gallery and Accademia Gallery housing masterpieces by Michelangelo, Botticelli, and more, making it a haven for art enthusiasts.\n",
      "\n",
      "3. St. Petersburg, Russia: St. Petersburg boasts the Hermitage Museum, one of the largest and most prestigious art museums in the world, showcasing a vast collection of art and cultural artifacts from around the globe.\n"
     ]
    }
   ],
   "source": [
    "response = chatgpt(\n",
    "        [\n",
    "            SystemMessage(content=high_level_behavior),\n",
    "            AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n",
    "            HumanMessage(content=\"Where should I travel next?\"),\n",
    "            SystemMessage(content=\"What do you enjoy doing?\"),\n",
    "            HumanMessage(content=\"I love going to Museums?\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f2cdd-0466-423f-8984-45a5e54585af",
   "metadata": {},
   "source": [
    "### **Text Embedding Model**\n",
    "\n",
    "When documents or string-variables are too long, things got quite complicated. \n",
    "\n",
    "**In order to be able to process them, we can embed and convert string variables into vectors** (a series of numbers that hold the semantic 'meaning' of your text).\n",
    "\n",
    "Mainly used when comparing different pieces of text or when dealing with huge texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd391e69-5df4-4de4-8625-c590fb372227",
   "metadata": {},
   "source": [
    "**TASK:**\n",
    "- First import the `Embeddings` model from langcgain.embeddings.\n",
    "- Define a text to embed. \n",
    "- Embed the text with the `.embed_query` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3980f040-5612-4b0b-9a2d-3b9e3c8b2ba4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 157,
    "lastExecutedAt": 1704817207570,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# 1. Import the embedding model\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# 2. Create an instance of the model\nembeddings = OpenAIEmbeddings()\n\n# 3. Define a text to embed\ntext = \"This is a webinar!\"\n\n# 4. Embed the text\ntext_embedding = embeddings.embed_query(text)\n\nprint (f\"Your embedding is length {len(text_embedding)}\")\nprint (f\"Here's a sample: {text_embedding[:5]}...\")",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is length 1536\n",
      "Here's a sample: [0.009987601079046726, -0.014433100819587708, 0.010184594430029392, -0.0038019861094653606, -0.01780826598405838]...\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the embedding model\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 2. Create an instance of the model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 3. Define a text to embed\n",
    "text = \"Hi! It's time to go to a Museum!\"\n",
    "\n",
    "# 4. Embed the text\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d5161-1b4c-48f6-b0b0-9604585178b7",
   "metadata": {},
   "source": [
    "### **Chains**\n",
    "\n",
    "Conversation chains in the context of Langchain are a concept involving the sequential linking of multiple conversational elements to build complex interactions. The idea is to streamline and enhance the conversation flow.\n",
    "\n",
    "The most basic chain is the `ConversationChain`. However, we will use the `load_qa_chain` to query questions about our documents, as its main function is optimized for this task. \n",
    "\n",
    "You can go check all available chains in the [LangChain Documentation.](https://python.langchain.com/docs/modules/chains/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "569cf2d8-d2df-4c63-8bd6-a37745686a13",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 412,
    "lastExecutedAt": 1704817272868,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chains import ConversationChain\n\nconversation = ConversationChain(llm=chatgpt)\nconversation.run(\"Hello!\")\n\n#from langchain.chains.question_answering import load_qa_chain \n\n#chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n#chain.run(input_documents = matches, question = enriched_query)"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=chatgpt)\n",
    "conversation.run(\"Hello!\")\n",
    "\n",
    "#from langchain.chains.question_answering import load_qa_chain \n",
    "\n",
    "#chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n",
    "#chain.run(input_documents = matches, question = enriched_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae7808-4897-4435-aa62-b29625f0c5aa",
   "metadata": {},
   "source": [
    "### Memory\n",
    "When interacting with a model, it is important to keep track of all interactions performed with it. \n",
    "\n",
    "To overcome these limitations, langchain implements different types of memories to use in your application.\n",
    "\n",
    "It is important to consider that storing all the interactions with the model can quickly escalate to a considerable amount of tokens to process every time we prompt the model. It is essential to bear in mind that ChatGPT has a token limit per interaction.\n",
    "\n",
    "You can learn more about memory [here]([https://towardsdatascience.com/custom-memory-for-chatgpt-api-artificial-intelligence-python-722d627d4d6d])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6dd11b7-b05e-435b-b8f2-27505a395c75",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 4966,
    "lastExecutedAt": 1704817328092,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.memory import ConversationSummaryBufferMemory\n\nmemory = ConversationSummaryBufferMemory(llm=chatgpt, max_token_limit=100)\n\nmemory.save_context({\"input\":  \"Can you recommend me where should I travel next?\"}, \n                    {\"output\": \"Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?\"})\n\nmemory.save_context({\"input\":  \"I love going to Museums\"}, \n                    {\"output\": \"Great then you should go to a cultural capital.\"})\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nconversation = ConversationChain(\n    llm=chatgpt, \n    memory = memory,\n    verbose=True\n)\n\nconversation.run(\"What cities do you recommend me?\")",
    "outputsMetadata": {
     "0": {
      "height": 353,
      "type": "stream"
     },
     "1": {
      "height": 337,
      "type": "stream"
     },
     "2": {
      "height": 97,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Can you recommend me where should I travel next?\n",
      "AI: Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?\n",
      "Human: I love going to Museums\n",
      "AI: Great then you should go to a cultural capital.\n",
      "Human: What cities do you recommend me?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"There are many cities around the world that are known for their rich cultural heritage and numerous museums. Some popular options include Paris, France, which is home to the Louvre Museum and the Musée d'Orsay; London, England, where you can visit the British Museum and the Tate Modern; and New York City, USA, which boasts the Metropolitan Museum of Art and the Museum of Modern Art. These cities offer a wide range of museums and cultural experiences for you to explore.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=chatgpt, max_token_limit=100)\n",
    "\n",
    "memory.save_context({\"input\":  \"Can you recommend me where should I travel next?\"}, \n",
    "                    {\"output\": \"Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?\"})\n",
    "\n",
    "memory.save_context({\"input\":  \"I love going to Museums\"}, \n",
    "                    {\"output\": \"Great then you should go to a cultural capital.\"})\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n",
    "                  temperature=0\n",
    "                 )\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chatgpt, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.run(\"What cities do you recommend me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c27ca-2d99-45d1-9d64-3436d095765e",
   "metadata": {},
   "source": [
    "### Dealing with Documents\n",
    "\n",
    "We are here to deal with documents... so LangChain provides a wide variety of elements to deal with them. \n",
    "\n",
    "One of the most important improvements of LangChain is that it allows us to upload documents and pass them to our model. \n",
    "We consider a document as an object that holds a piece of text and metadata (more information about that text)\n",
    "\n",
    "- Document class\n",
    "- Document Loader\n",
    "- Document Retriever\n",
    "- Text Splitter\n",
    "- Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd360d81-3068-44b3-b72b-451623776022",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "1. From langchain.schema import the `Document` class. \n",
    "2. Now define a document that has \n",
    "   - Text contained in page_content. \n",
    "   - Metada composed of document_id, document_source and document_create_time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d33238-ce16-45fd-b216-1b9166341193",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1704817584415,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# 1. Import the document class\nfrom langchain.schema import Document\n\n# 2. Define the document:\nDocument(\n         page_content=\"This is a dummy document\",\n         metadata={\n             'document_id' : 677,\n             'document_source' : \"mysource.pdf\",\n             'document_create_time' : \"01/06/2022\"\n                   })"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This is a dummy document', metadata={'document_id': 677, 'document_source': 'mysource.pdf', 'document_create_time': '01/06/2022'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Import the document class\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 2. Define the document:\n",
    "Document(\n",
    "         page_content=\"This is a dummy document\",\n",
    "         metadata={\n",
    "             #\n",
    "             #\n",
    "             #\n",
    "                   }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c33e2ea-572e-4fbd-a348-5bcf0c961a8b",
   "metadata": {},
   "source": [
    "#### Document Loaders\n",
    "\n",
    "Depending on where our data is stored, we will need a different type of loader:\n",
    "\n",
    "- The **Online Loader** is used for loading a document directly from the Internet. LangChain implements different types of loaders. For example, there is the `WikipediaLoader` that helps you loading Wikipedia pages or the `HNLoader` to take content directly from any HackerNews page.\n",
    "\n",
    "\n",
    "\n",
    "- The **Offline Loader** is used loading a document stored that are already installed in your machine. There are also different types of offline loaders such as the **HTML** loader for `.html` pages or the **PyPDFLoader** for `.pdf` documents.\n",
    "\n",
    "In this tutorial, we will see an example of Online Loader by using the `WikipediaLoader` and the `HNLoader`, and an example of Offline Loader by using the PyPDFLoader.\n",
    "\n",
    "You can find a list of the supported [LangChain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders) in the official documentation. Those Loaders are from external integrations, [native LangChain Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) can be found in the official documentation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2599ff89-9838-4341-91a9-424eb3c4a49d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5701,
    "lastExecutedAt": 1704817696213,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.document_loaders import WikipediaLoader\n \n# Load content from Wikipedia using WikipediaLoader\nloader = WikipediaLoader(\"Machine_learning\")\nwikipedia_data = loader.load() #It returns a list of documents\n\nwikipedia_data[0]"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field\\'s methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\\n\\n\\n== History and relationships to other fields ==\\n\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb\\'s model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.By the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to re-evaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entitie', metadata={'title': 'Machine learning', 'summary': \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    " \n",
    "# Load content from Wikipedia using WikipediaLoader\n",
    "loader = WikipediaLoader(\"Machine_learning\")\n",
    "wikipedia_data = loader.load() #It returns a list of documents\n",
    "\n",
    "wikipedia_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3dca600-e561-4654-b688-7cec585ae80d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1704817744183,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(\"\\nPage Content: \\n\", wikipedia_data[0].page_content)\nprint(\"\\nMeta Data: \\n\", wikipedia_data[0].metadata)",
    "outputsMetadata": {
     "0": {
      "height": 578,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page Content: \n",
      " Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\n",
      "The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\n",
      "\n",
      "\n",
      "== History and relationships to other fields ==\n",
      "\n",
      "The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.By the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to re-evaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entitie\n",
      "\n",
      "Meta Data: \n",
      " {'title': 'Machine learning', 'summary': \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPage Content: \\n\", wikipedia_data[0].page_content)\n",
    "print(\"\\nMeta Data: \\n\", wikipedia_data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5bc9d-5c18-4bb6-b30b-025da04e0862",
   "metadata": {},
   "source": [
    "**TASK:**\n",
    "\n",
    "Repeat the previous procedure using `HNLoader` and `PyPDFLoader`. \n",
    "1. Import the corresponding Loader from langchain.document_loaders. \n",
    "2. Initialize the loader indicating the source of data. \n",
    "    - HNLoader -> https://news.ycombinator.com/item?id=34422627\n",
    "    - PyPDFLoader -> Docs/attentions.pdf\n",
    "    - PyPDFDirectoryLoader -> Docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7b588e-8146-4df2-8e85-2e7d6d89cbb0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2520,
    "lastExecutedAt": 1704817928107,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Online Loader\nfrom langchain.document_loaders import HNLoader\nloader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\nhn_data = loader.load()\n\n# Load content from local PDFs\nfrom langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_data = loader.load()\n\n#We can use a directory loader to load more than one PDF at once. \n#loader = PyPDFDirectoryLoader(\"Docs/\")\n#pdf_directory_data = loader.load()"
   },
   "outputs": [],
   "source": [
    "# Online Loader\n",
    "from langchain.document_loaders import HNLoader\n",
    "# loader = \n",
    "# hn_data = \n",
    "\n",
    "# Load content from local PDFs\n",
    "\n",
    "#We can use a directory loader to load more than one PDF at once. \n",
    "#loader = PyPDFDirectoryLoader(\"Docs/\")\n",
    "#pdf_directory_data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29979d-f779-41e1-a35c-598770a03139",
   "metadata": {},
   "source": [
    "#### Text Splitter\n",
    "\n",
    "**Data Chunks and Model Tokenizer**\n",
    "\n",
    "To efficiently handle data when building an LLM-based application, data needs to be divided in portions. Those are the so-called data chunks and the chunk size is highly determinant in the quality of the chatbot.\n",
    "\n",
    "The tokenizer plays a crucial role in relation to data chunks when working with LLMs: \n",
    "- A **tokenizer is the tool used to convert text data into a format that can be processed by the model.**\n",
    "- Data is then stored in the vector stores in the tokenized format.\n",
    "\n",
    "To convert the original data into tokens and split it in data chunks, we will use the **LangChain Text Splitter**.\n",
    "\n",
    "If you are interested in more details about the tokenizer, the article [Unleashing the ChatGPT Tokenizer](https://medium.com/towards-data-science/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54) is for you!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a417e-7708-4d1e-bc99-0b0ba703dbe1",
   "metadata": {},
   "source": [
    "By using Langchain, we can highly customize how to split our data:\n",
    "- **Split by chunks**: The most general approach is to split your data into chunks of a concrete size. In the following example, we will take the data that we have already loaded (`wikipedia_data`, `hn_data` and `pdf_data`) and we will split it in portions of 200 characters. \n",
    "\n",
    "_What will happen if the split based on character count breaks a word?_\n",
    "\n",
    "There is the concept of \"chunk overlap\" that refers to a method where consecutive chunks of text share some common content. This technique is used to maintain context and coherence when a long document is divided into smaller parts due to the token limitations of LLMs. In this case, we will use a chunk size of 20 characters.\n",
    "\n",
    "So let's split the Wikipedia data we have just loaded: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df0ebc1-87a3-4755-a05d-4088db03b29d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 403,
    "lastExecutedAt": 1704818041060,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# Advanced method - Split by chunks ________________________________________________________________________\n# Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\nprint(\"\\nSPLITTING BY CHUNKS\")\nwikipedia_chunks = text_splitter.split_documents(wikipedia_data)\nprint(\"Wikipedia Data - Now you have {0} number of chunks.\".format(len(wikipedia_chunks)))",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     },
     "1": {
      "height": 57,
      "type": "stream"
     },
     "2": {
      "height": 77,
      "type": "stream"
     },
     "4": {
      "height": 77,
      "type": "stream"
     },
     "5": {
      "height": 57,
      "type": "stream"
     },
     "6": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPLITTING BY CHUNKS\n",
      "Wikipedia Data - Now you have 78 number of chunks.\n"
     ]
    }
   ],
   "source": [
    "# Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "# Tokenizer\n",
    "from transformers import GPT2TokenizerFast  \n",
    "\n",
    "# Advanced method - Split by chunks ________________________________________________________________________\n",
    "# Create function to count tokens\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size      = 200, \n",
    "    chunk_overlap   = 20,\n",
    "    length_function = count_tokens # It uses len() by default. \n",
    ")\n",
    "\n",
    "print(\"\\nSPLITTING BY CHUNKS\")\n",
    "wikipedia_chunks = text_splitter.split_documents(wikipedia_data)\n",
    "print(\"Wikipedia Data - Now you have {0} number of chunks.\".format(len(wikipedia_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb800f3-0b01-4bff-9291-1736cb5cdbe4",
   "metadata": {},
   "source": [
    "**TASK:**\n",
    "\n",
    "Generate the chunks for both `HNLoader` and `PyPDFLoader`. \n",
    "1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n",
    "2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "3. Define a count_tokens function that will allow us to count the tokens of out text. \n",
    "4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n",
    "5. Apply the command `.split_documents`to our data. \n",
    "\n",
    "You can define your own function for the HN data and use your the default function for the PDF Data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd61a742-dc5e-441a-a746-37d43012aceb",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 288,
    "lastExecutedAt": 1704818288170,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#_____________________________________________________________________PDF DATA\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast  \n \n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    chunk_overlap=20,\n    length_function=count_tokens\n)\n\n# 5 - Apply the .split_document command\npdf_chunks = text_splitter.split_documents(pdf_data)\nprint(\"PDF Data - Now you have {0} number of chunks.\".format(len(pdf_chunks)))\n\n\n#_____________________________________________________________________HN DATA\n# 3 - We use the default len, no need to do anything.\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    chunk_overlap=20\n)\n\nhn_chunks = text_splitter.split_documents(hn_data)\nprint(\"Online HN - Now you have {0} number of chunks.\".format(len(hn_chunks)))",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Data - Now you have 64 number of chunks.\n",
      "Online HN - Now you have 232 number of chunks.\n"
     ]
    }
   ],
   "source": [
    "#_____________________________________________________________________PDF DATA\n",
    "# 1 - Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "# 2 - Tokenizer\n",
    " \n",
    "# 3 - Create function to count tokens\n",
    "\n",
    "# 4 - Define the splitter\n",
    "# text_splitter = ____( )\n",
    "\n",
    "\n",
    "# 5 - Apply the .split_document command\n",
    "# pdf_chunks = text_splitter.\n",
    "# print(\"PDF Data - Now you have {0} number of chunks.\"_________))\n",
    "\n",
    "\n",
    "# HN DATA\n",
    "# 3 - We use the default len, no need to do anything.\n",
    "\n",
    "# 4 - Define the splitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad447dd1-d5a5-4b2e-91ce-ef41539ddf73",
   "metadata": {},
   "source": [
    "We can make sure that the chunking has been successful by visualizing the distribution of chunk sizes. \n",
    "Since we have selected a chunk size of 200, the majority of our chunks should have this lenght:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31777cc6-9a05-45d6-b1b6-01db6587270c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 309,
    "lastExecutedAt": 1704818302589,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Quick data visualization to ensure chunking was successful\n\n# Create a list of token counts\ntoken_counts = [count_tokens(chunk.page_content) for chunk in pdf_chunks]\n\n# Create a DataFrame from the token counts\ndf = pd.DataFrame({'Token Count': token_counts})\n\n# Create a histogram of the token count distribution\ndf.hist(bins=40, )\n\n# Show the plot\nplt.show()"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick data visualization to ensure chunking was successful\n",
    "\n",
    "# Create a list of token counts\n",
    "token_counts = [count_tokens(chunk.page_content) for chunk in pdf_chunks]\n",
    "\n",
    "# Create a DataFrame from the token counts\n",
    "df = pd.DataFrame({'Token Count': token_counts})\n",
    "\n",
    "# Create a histogram of the token count distribution\n",
    "df.hist(bins=40, )\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149bde7-39f5-4912-83f2-fb238e4d72a7",
   "metadata": {},
   "source": [
    "- **Split by pages**: If your data comes from documents organized in pages, there are methods that allow you to split data in pages to keep track of the page content. This method is specially useful when dealing with PDFs, as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87a8e84c-d658-4a7a-8144-d84fcb169928",
   "metadata": {
    "executionCancelledAt": 1704727738639,
    "executionTime": 1063,
    "lastExecutedAt": 1704708532500,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Simple method - Split by pages    ________________________________________________________________________\n# You need a PDF file in your environement. \nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_pages_chunks = loader.load_and_split()\npdf_pages_chunks\n\nprint(\"\\nSPLITTING BY PAGES\")\nprint(\"PDF Splited by Pages - You have {0} number of chunks.\".format(len(pdf_pages_chunks)))",
    "outputsMetadata": {
     "0": {
      "height": 77,
      "type": "stream"
     },
     "1": {
      "height": 57,
      "type": "stream"
     },
     "2": {
      "height": 137,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Simple method - Split by pages    ________________________________________________________________________\n",
    "# You need a PDF file in your environement. \n",
    "loader = PyPDFLoader(\"Docs/attentions.pdf\")\n",
    "pdf_pages_chunks = loader.load_and_split()\n",
    "pdf_pages_chunks\n",
    "\n",
    "print(\"\\nSPLITTING BY PAGES\")\n",
    "print(\"PDF Splited by Pages - You have {0} number of chunks.\".format(len(pdf_pages_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06933573-24eb-4ba2-bf05-4410912be27e",
   "metadata": {},
   "source": [
    "### Vector Stores\n",
    "\n",
    "Vector stores, also known as vector databases, are specialized types of databases designed to efficiently handle and manipulate high-dimensional vector data. In our case, we will store the tokenized and splitted content, e.g., the data chunks in the format that LLMs can process.\n",
    "\n",
    "There are different types of vector stores. Depending on the storage of the data, we can classify them as:\n",
    "- **Local Vector Stores**: This type of databases store the information in your local system. As an example of Local Vector Store, we will use FAISS.\n",
    "- **Online Vector Stores**: This type of databases store the information in the cloud. We will use Pinecone as out preferred option for Online Vector Stores.\n",
    "\n",
    "FAISS - EXAMPLE OF LOCAL VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7dc17d0-275f-4376-86e1-11661a2dd1ef",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 4673,
    "lastExecutedAt": 1704818404491,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.vectorstores import FAISS  # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Get embedding model\nembeddings = OpenAIEmbeddings()\n\n# OPTION 1: FAISS (Facebook AI Similarity Search) Local _______________________________________________________________________________________\n# Create vector database\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)",
    "outputsMetadata": {
     "0": {
      "height": 580,
      "type": "stream"
     },
     "1": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS  # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Get embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# OPTION 1: FAISS (Facebook AI Similarity Search) Local _______________________________________________________________________________________\n",
    "# Create vector database\n",
    "db_FAISS = FAISS.from_documents(pdf_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4faca6-6ae9-453c-90d0-b31fc75a7872",
   "metadata": {},
   "source": [
    "PINECONE - EXAMPLE OF ONLINE VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pinecone library\n",
    "from pinecone import Pinecone #We need the Pinecone library to initialize our connection.\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# Initialize a Pinecone client with your API key\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "\n",
    "# Create a new pinecone index\n",
    "#pc.create_index(name=\"langchain\", \n",
    "#                dimension=1536, \n",
    "#                metric=\"cosine\",\n",
    "#                spec=ServerlessSpec(\n",
    "#                    cloud=\"aws\",\n",
    "#                    region=\"us-east-1\"\n",
    "#                    )\n",
    "#               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "776ae3bd-a6d2-4066-881a-dd08cc9f9ede",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2496,
    "lastExecutedAt": 1704818462367,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pinecone  #We need the Pinecone library to initialize our connection.\nfrom langchain.vectorstores import Pinecone # for the vector database part -- Pinecone is cloud-\n# OPTION 2: PINECONE Online\n     \n# We initialize pinecone\npinecone.init(      \n\tapi_key=os.getenv(\"PINECONE_API_KEY\"),      \n\tenvironment=os.getenv(\"PINECONE_ENV_KEY\")     \n) \n\n# Create a new pinecone index\n#pinecone.create(name=\"langchain\", dimension=1536, metric=\"cosine\")\n\n# We define the name of our index (in case the index is already created)\nindex_name = \"langchain\"\n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`. We use the Pinecone library of LangChain\ndb_Pinecone = Pinecone.from_documents(pdf_chunks, embeddings, index_name=index_name)"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone # for the vector database part -- Pinecone is cloud-\n",
    "# OPTION 2: PINECONE Online\n",
    "\n",
    "# We define the name of our index (in case the index is already created)\n",
    "index_name = \"langchain\"\n",
    "\n",
    "# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`. We use the Pinecone library of LangChain\n",
    "db_Pinecone = Pinecone.from_documents(pdf_chunks, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6151d-08d3-4c8f-8a7a-643c101bd701",
   "metadata": {},
   "source": [
    "### Natural Language Retrieval\n",
    "We first start performing a semantic search within our Vector DataBase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "826da9fb-03ba-4101-b793-3b9ac3203d79",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 900,
    "lastExecutedAt": 1704818532774,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=2)\nprint(matches)",
    "outputsMetadata": {
     "0": {
      "height": 458,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com', metadata={'source': 'Docs/attentions.pdf', 'page': 0, 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com'}), Document(page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'Docs/attentions.pdf', 'page': 13, 'text': '.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'})]\n"
     ]
    }
   ],
   "source": [
    "# LOCAL - FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain \n",
    "\n",
    "# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\n",
    "db_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n",
    "\n",
    "# We can define how many similarities we want to get back by defining the variable k\n",
    "query = \"Can you please tell me all the autors of the article Attention is all you need?\"\n",
    "matches = db_FAISS.similarity_search(query, k=2)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    print(\"\\n\", match.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17489dca-2a53-40ab-9b12-13a65e1f8c53",
   "metadata": {},
   "source": [
    "In the above section, we have seen how to retrieve the coincidences of you query in the documents in our vector store. Nevertheless, the output is a bit difficult to read. We can leverage the usage of LLMs by feeding the coincidences in our vector store to an LLM and let it generate a response in Natural Language using the additional information from our documents. We can do so by using the so-called **[LangChain Chains](https://python.langchain.com/docs/expression_language/get_started)**, and **[LangChain FAISS integration](https://python.langchain.com/docs/integrations/vectorstores/faiss/)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccc9684f-f666-473f-8a48-9c8f53e413ab",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2981,
    "lastExecutedAt": 1704818563496,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# ONLINE - PINECONE\n\n# 1. Define our query of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 2. Perform the semantic search in our vector database with the similarity_search command.  \nmatches = db_Pinecone.similarity_search(query, k=2)\n\n# 3. Define a load_qa_chain.\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Execute the chain with the prompt and the matches. \nchain.run(input_documents=matches, question = query)",
    "outputsMetadata": {
     "0": {
      "height": 257,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors of the article \"Attention Is All You Need\" are:\\n\\n1. Ashish Vaswani\\n2. Noam Shazeer\\n3. Niki Parmar\\n4. Jakob Uszkoreit\\n5. Llion Jones\\n6. Aidan N. Gomez\\n7. Łukasz Kaiser\\n8. Illia Polosukhin'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLINE - PINECONE\n",
    "\n",
    "# 1. Define our query of interest. \n",
    "query = \"Can you please tell me all the autors of the article Attention is all you need?\"\n",
    "\n",
    "# 2. Perform the semantic search in our vector database with the similarity_search command.  \n",
    "matches = db_Pinecone.similarity_search(query, k=2)\n",
    "\n",
    "# 3. Define a load_qa_chain.\n",
    "chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n",
    "\n",
    "# 4. Execute the chain with the prompt and the matches. \n",
    "chain.run(input_documents=matches, question = query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cbc3f-2d98-4085-8fa4-827cf7589e24",
   "metadata": {},
   "source": [
    "**TASK:**\n",
    "Repeat the previous procedure using FAISS.\n",
    "1. Define a query of your interest. For example, \"Can you please tell me all the autors of the article Attention is all you need?\"\n",
    "2. Use the db_Pinecone database together with the `.similarity_search`command to perform a semantic search. \n",
    "3. Define a `load_qa_chain`and pass the matches together with the query to obtain a NLP based answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d164cac5-2244-4582-b567-e9ae4461afdd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2611,
    "lastExecutedAt": 1704818697699,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=4)\n\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\nchain.run(input_documents=matches, question = query)\n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors of the article \"Attention Is All You Need\" are:\\n\\n1. Ashish Vaswani\\n2. Noam Shazeer\\n3. Niki Parmar\\n4. Jakob Uszkoreit\\n5. Llion Jones\\n6. Aidan N. Gomez\\n7. Łukasz Kaiser\\n8. Illia Polosukhin'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOCAL - FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain \n",
    "\n",
    "# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\n",
    "\n",
    "# 1. Define a query of your interest. For example, \"Can you please tell me all the autors of the article Attention is all you need?\"\n",
    "# We can define how many similarities we want to get back by defining the variable k\n",
    "query = \"Can you please tell me all the autors of the article Attention is all you need?\"\n",
    "# 2. Use the db_Pinecone database together with the `.similarity_search`command to perform a semantic search. \n",
    "# matches = \n",
    "\n",
    "# 3. Define a `load_qa_chain`and pass the matches together with the query to obtain a NLP based answer. \n",
    "#chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad7d06-178b-467b-979b-8976582012a7",
   "metadata": {},
   "source": [
    "### Indexes and Metadata\n",
    "When we upload data to our vector database, there is metadata that allows us to understand where the data is coming from. \n",
    "When dealing with PDFs, the source information allows us to know what pdf and page the info is coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97d1db0b-b343-4836-b43b-8e4ab735c033",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 174,
    "lastExecutedAt": 1704818705089,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#from langchain.embeddings import OpenAIEmbeddings\n#from langchain.indexes import index\n\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\n\nprint(\"______________________________________ THIRD MATCH\")\n\nprint(\"We can get the chunk text content and get: \\n\", matches[3].page_content)\nprint(\"\\nWe can get the chunk metadata and get: \\n\", matches[3].metadata)\nprint(\"\\nThe source of our match is: \\n\" , matches[3].metadata[\"source\"], \"and page\", matches[3].metadata[\"page\"])",
    "outputsMetadata": {
     "0": {
      "height": 563,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________ THIRD MATCH\n",
      "We can get the chunk text content and get: \n",
      " our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "\n",
      "We can get the chunk metadata and get: \n",
      " {'source': 'Docs/attentions.pdf', 'page': 0, 'text': 'our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'}\n",
      "\n",
      "The source of our match is: \n",
      " Docs/attentions.pdf and page 0\n"
     ]
    }
   ],
   "source": [
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "#from langchain.indexes import index\n",
    "\n",
    "query = \"Who created transformers?\"\n",
    "matches = db_FAISS.similarity_search(query)\n",
    "\n",
    "print(\"______________________________________ \\n THIRD MATCH\")\n",
    "\n",
    "print(\"We can get the chunk text content and get: \\n\", matches[3].page_content)\n",
    "print(\"\\nWe can get the chunk metadata and get: \\n\", matches[3].metadata)\n",
    "print(\"\\nThe source of our match is: \\n\" , matches[3].metadata[\"source\"], \"and page\", matches[3].metadata[\"page\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e5f032-6e4d-40b2-b851-ccb9c86fc097",
   "metadata": {},
   "source": [
    "Now it is the time to put it all together and generate a simple pipeline to query our documents using a LLM model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0282a5-73c9-44b8-87fd-eb175d676748",
   "metadata": {},
   "source": [
    "# PART 2: Loading and processing our documents\n",
    "\n",
    "\n",
    "\n",
    "PyPDFDirectoryLoader allows us to upload multiple PDFs at once. In our case, we have two PDFs in the Docs directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db047f-2475-4738-a726-458a5799a21e",
   "metadata": {},
   "source": [
    "## **STEP 1 - LOADER**\n",
    "\n",
    "Use the `PDFDirectoryLoader` to upload all PDFs contained within the the Docs folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a374a538-47c8-4679-a5d6-6c3e4a8a208c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 13294,
    "lastExecutedAt": 1704818828591,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#STEP 1 - LOADER\nfrom langchain.document_loaders import PyPDFDirectoryLoader\n\nloader = PyPDFDirectoryLoader(\"Docs/\")\n\ndata = loader.load()"
   },
   "outputs": [],
   "source": [
    "#STEP 1 - LOADER\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"Docs/\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aebdc5-1686-4401-89d2-3b59546f8d3c",
   "metadata": {},
   "source": [
    "## **STEP 2 - CHUNKING**\n",
    "\n",
    "Generate the chunks for the PDFs contained in the directory. \n",
    "1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n",
    "2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "3. Define a count_tokens function that will allow us to count the tokens of out text. \n",
    "4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n",
    "5. Apply the command `.split_documents`to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "198312a1-bd33-4922-b5fd-1e402cd10c91",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1378,
    "lastExecutedAt": 1704818978129,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#STEP 2 - CHUNKING OUR DATA\n#_____________________________________________________________________PDFs Data\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast \n\n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\n# 5 - Apply the .split_document command\nchunks = text_splitter.split_documents(data)\nprint(\"Multiple PDFs - Now you have {0} number of chunks.\".format(len(chunks)))",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     },
     "1": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple PDFs - Now you have 610 number of chunks.\n"
     ]
    }
   ],
   "source": [
    "#STEP 2 - CHUNKING OUR DATA\n",
    "#_____________________________________________________________________PDFs Data\n",
    "# 1 - Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "# 2 - Tokenizer\n",
    "from transformers import GPT2TokenizerFast \n",
    "\n",
    "# 3 - Create function to count tokens\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# 4 - Define the splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size      = 200, \n",
    "    chunk_overlap   = 20,\n",
    "    length_function = count_tokens # It uses len() by default. \n",
    ")\n",
    "\n",
    "# 5 - Apply the .split_document command\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(\"Multiple PDFs - Now you have {0} number of chunks.\".format(len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f548885-aee9-4b4a-bf1d-e996ed02c74d",
   "metadata": {},
   "source": [
    "## **STEP 3 - EMBEDD AND UPLOAD THE DATA INTO A VECTORSTORE**\n",
    "\n",
    "**TASK**\n",
    "- Upload the data into the FAISS vector store using the `from_documents`command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04a6a94c-153d-438a-978b-df189a33fa6c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 4211,
    "lastExecutedAt": 1704819033112,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# STEP 3 - EMBEDDING AND UPLOAD DATA INTO OUR VECTORSTORE\n\n# ___________________________________________________________________________ LOCAL VERSION\n\n# 1. Create vector database with FAISS\ndb_FAISS = FAISS.from_documents(chunks, embeddings)\n\n# Check similarity search is working\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\nprint(\"We found {0} number of similarities.\".format(len(matches)))\nfor match in matches:\n    print(\"\\n\", match.page_content)",
    "outputsMetadata": {
     "0": {
      "height": 578,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 4 number of similarities.\n",
      "\n",
      " To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n",
      "of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n",
      "\n",
      " vision (Dosovitskiy et al., 2021), and many other areas (Oord et al., 2018; Jumper et al., 2021). Prior\n",
      "works, such as CuBERT (Kanade et al., 2020), CodeBERT (Feng et al., 2020), PyMT5 (Clement et al.,\n",
      "2020), and CodeT5 (Wang et al., 2021), have applied transformers towards code understanding but\n",
      "these mostly focus on code retrieval, classiﬁcation, and program repair. Several recent and concurrent\n",
      "efforts explore using large language models for program synthesis (Chen et al., 2021; Austin et al.,\n",
      "2021; Li et al., 2022; Fried et al., 2022) and its effectiveness (Vaithilingam et al., 2022). While they\n",
      "focus on generating code in a single turn, we propose to factorize the speciﬁcations into multiple turns\n",
      "\n",
      " The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "\n",
      " illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n"
     ]
    }
   ],
   "source": [
    "# STEP 3 - EMBEDDING AND UPLOAD DATA INTO OUR VECTORSTORE\n",
    "\n",
    "# LOCAL VERSION\n",
    "\n",
    "# 1. Create vector database with FAISS from chunks documents and embeddings\n",
    "# db_FAISS = \n",
    "\n",
    "# Check similarity search is working\n",
    "query = \"Who created transformers?\"\n",
    "# matches = \n",
    "print(\"We found {0} number of similarities.\".format(len(matches)))\n",
    "for match in matches:\n",
    "    print(\"\\n\", match.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afb3bf-6c25-4562-b9c7-f32d17729cf9",
   "metadata": {},
   "source": [
    "# PART 3: Talking with our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d54ac1-c3ff-42c3-a9c8-151548914489",
   "metadata": {},
   "source": [
    "## STEP 4 - DEFINE A CHAIN AND PERFORM THE SIMILARITY SEARCH\n",
    "Generating a simple pipeline to query our documents with a load_qa_chain. \n",
    "\n",
    "**TASK**\n",
    "1. Import the `load_qa_chain`from the langchain.chains.question_answering library. \n",
    "2. Define a prompt of interest, like: \"Can you please tell me all the autors of the article Attention is all you need?\"\n",
    "3. Define the chain.\n",
    "4. Perform a semantic search with the `.similarity_search`. \n",
    "5. Execute the chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85627ed4-89b0-475c-b7ea-caba04fce893",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1844,
    "lastExecutedAt": 1704819185918,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. Execute the chain to obtain a NLP based response. \nresponse = chain.run(input_documents = matches, question = query)\nprint(response)",
    "outputsMetadata": {
     "0": {
      "height": 227,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the article \"Attention Is All You Need\" are:\n",
      "\n",
      "1. Ashish Vaswani\n",
      "2. Noam Shazeer\n",
      "3. Niki Parmar\n",
      "4. Jakob Uszkoreit\n",
      "5. Llion Jones\n",
      "6. Aidan N. Gomez\n",
      "7. Łukasz Kaiser\n",
      "8. Illia Polosukhin\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the load_qa_chain\n",
    "from langchain.chains.question_answering import load_qa_chain \n",
    "\n",
    "# 2. Define a prompt of interest. \n",
    "query = \"Can you please tell me all the autors of the article Attention is all you need?\"\n",
    "\n",
    "# 3. Define the chain\n",
    "chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n",
    "\n",
    "# 4. Perform a similarity search. \n",
    "matches = db_FAISS.similarity_search(query, k=1)\n",
    "\n",
    "# 5. Execute the chain to obtain a NLP based response. \n",
    "response = chain.run(input_documents = matches, question = query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570da7e-07ca-4dbd-a390-0f74aeabc044",
   "metadata": {},
   "source": [
    "Now that we already have a working pipeline to query our documents, we want to understand where our data is coming from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9445c4e-6c17-4a2f-8a5e-0456873c8ac6",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3750,
    "lastExecutedAt": 1704819277172,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)",
    "outputsMetadata": {
     "0": {
      "height": 269,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the article \"Attention Is All You Need\" are:\n",
      "\n",
      "1. Ashish Vaswani - Google Brain (source: attentions.pdf, page 0)\n",
      "2. Noam Shazeer - Google Brain (source: attentions.pdf, page 0)\n",
      "3. Niki Parmar - Google Research (source: attentions.pdf, page 0)\n",
      "4. Jakob Uszkoreit - Google Research (source: attentions.pdf, page 0)\n",
      "5. Llion Jones - Google Research (source: attentions.pdf, page 0)\n",
      "6. Aidan N. Gomez - University of Toronto (source: attentions.pdf, page 0)\n",
      "7. Łukasz Kaiser - Google Brain (source: attentions.pdf, page 0)\n",
      "8. Illia Polosukhin - (source: attentions.pdf, page 0)\n",
      "\n",
      "Please note that the information provided is based on the given context.\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the load_qa_chain\n",
    "from langchain.chains.question_answering import load_qa_chain \n",
    "\n",
    "# 2. Define a prompt of interest. \n",
    "query = \"Can you please tell me all the autors of the article Attention is all you need?\"\n",
    "\n",
    "# 3. Define the chain\n",
    "chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n",
    "\n",
    "# 4. Perform a similarity search. \n",
    "matches = db_FAISS.similarity_search(query, k=1)\n",
    "\n",
    "# 5. We define both the text and the metadata obtain from the semantic search.\n",
    "input_text = [x.page_content for x in matches]\n",
    "input_metadata= [x.metadata for x in matches]\n",
    "\n",
    "# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \n",
    "meta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n",
    "\n",
    "# 7. We define an enriched query with the initial prompt and the metadata prompt. \n",
    "enriched_query = query + meta_data_enriching\n",
    "\n",
    "# 8. We execute the chain. \n",
    "response = chain.run(input_documents = matches, question = enriched_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfc58e-6995-4c1d-85b6-1cf185d480df",
   "metadata": {},
   "source": [
    "Try to ask the model something that is completely out of scope, and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7812e8d8-e650-4444-b616-36834abe6c23",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1558,
    "lastExecutedAt": 1704819297846,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"What are the main problems to cook with olive oil?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have the information you're looking for. The provided information is from a PDF document titled \"codegen.pdf\" on page 5.\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the load_qa_chain\n",
    "from langchain.chains.question_answering import load_qa_chain \n",
    "\n",
    "# 2. Define a prompt of interest. \n",
    "query = \"What are the main problems to cook with olive oil?\"\n",
    "\n",
    "# 3. Define the chain\n",
    "chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n",
    "\n",
    "# 4. Perform a similarity search. \n",
    "matches = db_FAISS.similarity_search(query, k=1)\n",
    "\n",
    "# 5. We define both the text and the metadata obtain from the semantic search.\n",
    "input_text = [x.page_content for x in matches]\n",
    "input_metadata= [x.metadata for x in matches]\n",
    "\n",
    "# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \n",
    "meta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n",
    "\n",
    "# 7. We define an enriched query with the initial prompt and the metadata prompt. \n",
    "enriched_query = query + meta_data_enriching\n",
    "\n",
    "# 8. We execute the chain. \n",
    "response = chain.run(input_documents = matches, question = enriched_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aeec8e-842f-4505-b895-8818cc7021fb",
   "metadata": {},
   "source": [
    "Try other queries and talk with your documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c4b26ec-dc92-49bc-9955-2516e8a0cca6",
   "metadata": {
    "executionCancelledAt": 1704727738652,
    "executionTime": 49,
    "lastExecutedAt": 1704708572393,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def asking_your_model(query, k):\n    # Define the chain\n    chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n    #Perform a similarity search. \n    matches = db_FAISS.similarity_search(query, k=k)\n    #We define both the text and the metadata obtain from the semantic search.\n    input_text = [x.page_content for x in matches]\n    input_metadata= [x.metadata for x in matches]\n    #We define a metadata prompt with the metadata and ask the model to explicitily state the source. \n    meta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in       the response\".format(input_metadata) \n    #We define an enriched query with the initial prompt and the metadata prompt. \n    enriched_query = query + meta_data_enriching\n    #We execute the chain. \n    response = chain.run(input_documents = matches, question = enriched_query)\n    return response\n    "
   },
   "outputs": [],
   "source": [
    "def asking_your_model(query, k):\n",
    "    # Define the chain\n",
    "    chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n",
    "    #Perform a similarity search. \n",
    "    matches = db_FAISS.similarity_search(query, k=k)\n",
    "    #We define both the text and the metadata obtain from the semantic search.\n",
    "    input_text = [x.page_content for x in matches]\n",
    "    input_metadata= [x.metadata for x in matches]\n",
    "    #We define a metadata prompt with the metadata and ask the model to explicitily state the source. \n",
    "    meta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in       the response\".format(input_metadata) \n",
    "    #We define an enriched query with the initial prompt and the metadata prompt. \n",
    "    enriched_query = query + meta_data_enriching\n",
    "    #We execute the chain. \n",
    "    response = chain.run(input_documents = matches, question = enriched_query)\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64a2c499-6ef6-424d-b58d-fbd571dd0522",
   "metadata": {
    "executionCancelledAt": 1704727738653,
    "executionTime": 2198,
    "lastExecutedAt": 1704708574592,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is functional correctness?\"\nresponse = asking_your_model(query, k=4)\nprint(response)",
    "outputsMetadata": {
     "0": {
      "height": 97,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain \n",
    "\n",
    "# Check similarity search is working\n",
    "query = \"What is functional correctness?\"\n",
    "response = asking_your_model(query, k=4)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6eb01de-7644-45a0-a88c-f290ee351640",
   "metadata": {
    "executionCancelledAt": 1704727738654,
    "executionTime": 3163,
    "lastExecutedAt": 1704708577755,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is the multi-head attention in a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)",
    "outputsMetadata": {
     "0": {
      "height": 217,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain \n",
    "\n",
    "# Check similarity search is working\n",
    "query = \"What is the multi-head attention in a transformer?\"\n",
    "response = asking_your_model(query, k=4)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0925bf69-fc5b-47d8-8f7d-322dab181ae3",
   "metadata": {
    "executionCancelledAt": 1704727738655,
    "executionTime": 3217,
    "lastExecutedAt": 1704708580972,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What are the main components of a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)",
    "outputsMetadata": {
     "0": {
      "height": 137,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain \n",
    "\n",
    "# Check similarity search is working\n",
    "query = \"What are the main components of a transformer?\"\n",
    "response = asking_your_model(query, k=4)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
